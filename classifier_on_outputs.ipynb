{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###%matplotlib widget\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from pprint import pprint\n",
    "from matplotlib.lines import Line2D\n",
    "from glob import glob \n",
    "import sys\n",
    "\n",
    "from scipy.special import comb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "def get_gen_ind(txt):\n",
    "    s = (txt.split('.')[-2]).split('_')[-2:]\n",
    "    print(txt.split('.')[-2])\n",
    "    print(txt, s)\n",
    "    return int(s[0][3:]), int(s[1][3:])\n",
    "\n",
    "def filter_spikes(spikes, start_t, end_t):\n",
    "    f = []\n",
    "    for i, times in enumerate(spikes):\n",
    "        if len(times) == 0:\n",
    "            f.append([])\n",
    "            continue\n",
    "        \n",
    "        ts = np.array(times)\n",
    "        whr = np.where(np.logical_and(start_t <= ts, ts < end_t))[0]\n",
    "        \n",
    "        if len(whr) == 0:\n",
    "            f.append([])\n",
    "            continue\n",
    "        \n",
    "        f.append(ts[whr].tolist())\n",
    "    \n",
    "    return f\n",
    "            \n",
    "def spikes_to_classes(spikes, sample_dt, sample_indices):\n",
    "    classes = []\n",
    "    for times in spikes:\n",
    "        l = []\n",
    "        for t in times:\n",
    "            idx = int(t // sample_dt)\n",
    "            ind_idx = sample_indices[ idx ] - 1\n",
    "#             print(t, idx, ind_idx)\n",
    "            l.append(ind_idx)\n",
    "        classes.append(l)\n",
    "    return classes\n",
    "    \n",
    "\n",
    "def spikes_to_vectors(spikes, start_t, end_t, sample_dt, labels, n_classes, n_out):\n",
    "    d = {l: [] for l in range(n_classes)}\n",
    "    counts = [0 for l in range(n_classes)]\n",
    "    for sample, st in enumerate(np.arange(start_t, end_t + 1, sample_dt)):\n",
    "\n",
    "        et = st + sample_dt\n",
    "        idx = int(st // sample_dt)\n",
    "        \n",
    "        c = labels[ idx ] - 1\n",
    "        counts[c] += 1\n",
    "        \n",
    "        v = np.zeros(n_out)\n",
    "        for i, times in enumerate(spikes):\n",
    "            if len(times) == 0:\n",
    "                continue\n",
    "            \n",
    "            ts = np.asarray(times)\n",
    "            whr = np.where(np.logical_and(st <= ts, ts < et))[0]\n",
    "\n",
    "            if len(whr):\n",
    "                v[i] = 1\n",
    "        d[c].append(v)\n",
    "        \n",
    "    print(counts)\n",
    "\n",
    "    return d\n",
    "\n",
    "def accum_vectors(vecs):\n",
    "    d = {k: np.zeros_like(vecs[k][0], dtype='uint')\\\n",
    "                                         for k in vecs}\n",
    "    \n",
    "    for k in vecs:\n",
    "        for v in vecs[k]:\n",
    "            d[k][:] = np.logical_or(d[k], np.asarray(v) > 0)\n",
    "    return d\n",
    "\n",
    "def accuracy(centroids, tests):\n",
    "    accs = {}\n",
    "    for cls, vs in tests.items():\n",
    "        cols = {}\n",
    "        for k, ctr in centroids.items():\n",
    "            cols[k] = [int(np.sum(np.logical_and(ctr, v))) for v in vs]\n",
    "\n",
    "        accs[cls] = cols\n",
    "\n",
    "    return accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/L2L-OMNIGLOT/run_results/data_gen00000_ind00005\n",
      "./L2L-OMNIGLOT/run_results/data_gen00000_ind00005.npz ['gen00000', 'ind00005']\n",
      "{1: 25, 2: 31, 3: 37, 4: 23, 5: 57, 6: 39, 7: 33, 8: 29, 9: 23, 10: 25, 11: 25, 12: 38, 13: 36, 14: 27}\n",
      "[ 7  4  5  5 13  5 13  2  5 13  7  8  6  6  2  8  4  9  5  5  1  4 14  3\n",
      "  5 14  7  5 11 11 12 10  7 14  4 13 11  9  5 13 13  5  6  1 13  2  5  4\n",
      "  5 10 12 12  8  7  3  7  8  5  6 13  7  3  4 10 13  6  9 11  6  7 11  3\n",
      "  6  3 10  1  8  4  2  6  6  6 13  2 14 13  2  8  7  8  1  1 10  2 14 13\n",
      " 10 13 13  2  6  8 12  5  4  7  3  2 11  1  8 13  4 12  2 14  8  5 11  2\n",
      " 12  8  5  4  6  5  7  7  3  6  7 13  3  1 14  4 11 12  7  4  8 10  4  6\n",
      " 13  2  5 12 13  5  3 14  5 14  8  6  5  4  7  2  5 14  4  9  2  7 14 12\n",
      "  1  3  5  7 13  5  5  7 11  9  1 13  7  8  4 10  1  6  1  2  5  7 13 13\n",
      " 10  3  5 10  1  3  3  5  9  2 13  5 12  3 12 12  2 11 11 13  7  7  3  6\n",
      "  3  1 13 12  6 10  2  6  1  5 11 10  6  6  3  9 12  3  8  5 14 14 12  9\n",
      "  8 12 14  6  3 14  6  5  8 12  4  6  9  6  8  5 12 14  6  5  9 14  3 11\n",
      " 11 14  7 13  9  3  2  5 13  6  5 12 11  3  6  2 12  5  2 12 11  9  2  5\n",
      " 13 12  3 12  8 12  9 10  1 10  7  7 12  8  7  9 11 13 11  9  9  6  8 14\n",
      " 10  5  7 10  5  8  6  2  8  4 13  2  1  7 14  2  3 10  5  7  9  8 12  8\n",
      "  5 13  5  3  5  3  7 12 13 11 12  4  4  1  7  5  3 10  6 14  1  5  5  2\n",
      " 12  3  5  5  3  9  2 14 12  1  1 12 10  9  2  7  2  3 13  5 13  3 11  8\n",
      "  6  6 12 11  6  4  5  9 10 14  6  9 10  1 14 10  1  5  3 11  6  2 13  5\n",
      "  6  4  3  7  5 14 11 12  3  3  6  3 12 10  1 11  6  1 12  8  4  5  8  9\n",
      " 14 13  8 10 11 10  1  9  3 12  2 12 14 12  5  5]\n",
      "22456 21952 22400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 32, 37, 23, 57, 39, 33, 29, 23, 25, 25, 38, 36, 27]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-78dcc5d255bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrain_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtrain_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtrain_accums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccum_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-78dcc5d255bd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtrain_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtrain_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtrain_accums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccum_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "result_files = sorted(glob('./L2L-OMNIGLOT/run_results/*.npz'))\n",
    "# result_files = sorted(glob('/home/gp283/titan/run_results/*.npz'))\n",
    "tmp = np.load(result_files[0], allow_pickle=True)\n",
    "\n",
    "class_data = {}\n",
    "train_vecs_single = {}\n",
    "train_vecs_accum = {}\n",
    "accuracies = {}\n",
    "for rf in result_files[-1:]:\n",
    "    \n",
    "    try:\n",
    "        tmp = np.load(rf, allow_pickle=True)\n",
    "        data.clear()\n",
    "        for k in tmp:\n",
    "            try:\n",
    "                data[k] = tmp[k].item()\n",
    "            except:\n",
    "                data[k] = tmp[k]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    gen, ind = get_gen_ind(rf)\n",
    "    labels  = data['input']['labels']\n",
    "    \n",
    "    \n",
    "    duration = data['params']['sim']['duration']\n",
    "    n_test = data['params']['sim']['test_per_class']\n",
    "    sample_dt = data['params']['sim']['sample_dt']\n",
    "    n_samples = data['params']['sim']['samples_per_class']\n",
    "    n_class = data['params']['sim']['num_classes']\n",
    "    n_train = 2 * n_samples # epochs * samples\n",
    "    n_out = data['params']['sim']['output_size']\n",
    "    \n",
    "    \n",
    "    total_lbl = len(labels)\n",
    "    end = total_lbl -  (n_class * n_test)\n",
    "    start = end - (n_class * n_train)\n",
    "    unique, counts = np.unique(labels[start : end], return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(labels[start : end])\n",
    "    print(total_lbl, start, end)\n",
    "    plt.close('all')\n",
    "    fig = plt.figure(figsize=(15, 3))\n",
    "    ax = plt.subplot(1, 4, 1)\n",
    "    im = plt.hist(labels, bins=n_class)\n",
    "\n",
    "    ax = plt.subplot(1, 4, 2)\n",
    "    im = plt.hist(labels[start:], bins=n_class)\n",
    "    \n",
    "    ax = plt.subplot(1, 4, 3)\n",
    "    im = plt.hist(labels[start:end], bins=n_class)\n",
    "    \n",
    "    ax = plt.subplot(1, 4, 4)\n",
    "    im = plt.hist(labels[end:], bins=n_class)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    end_t = duration - sample_dt * n_class * n_test\n",
    "    start_t = end_t - sample_dt * n_class * n_train\n",
    "    \n",
    "#     print(start_t, end_t)\n",
    "    spikes = filter_spikes(data['recs']['output'][0]['spikes'], start_t, end_t)\n",
    "#     print(spikes)\n",
    "#     classes = spikes_to_classes(spikes, sample_dt, labels)\n",
    "    vecs = spikes_to_vectors(spikes, start_t, end_t, sample_dt, labels, n_class, n_out)\n",
    "    # not every set has the same number so reduce\n",
    "    n_train = 50\n",
    "\n",
    "    rand_class = np.tile(np.arange(n_class), n_train)\n",
    "    train_id = np.random.choice(len(rand_class), size=len(rand_class), replace=False)\n",
    "    samp = train_id // n_class\n",
    "    train_class = train_id % n_class\n",
    "    train_vecs = [vecs[c][samp[i]].copy() for i, c in enumerate(train_class)]\n",
    "    train_accums = accum_vectors(vecs)\n",
    "    \n",
    "    assert (n_class * n_train) == len(train_vecs), 'Not the same number of vectors'\n",
    "\n",
    "\n",
    "    clf = KNeighborsClassifier()\n",
    "#     clf = linear_model.LogisticRegression()\n",
    "    clf.fit(train_vecs, train_class)\n",
    "    \n",
    "    v = data['analysis']['individual_per_class']['vectors']\n",
    "    rand_class = np.tile(np.arange(n_class), n_test)\n",
    "    test_id = np.random.choice(len(rand_class), size=len(rand_class), replace=False)\n",
    "    samp = test_id // n_class\n",
    "    test_class = test_id % n_class\n",
    "    test_vecs = [v[c][samp[i]].copy() for i, c in enumerate(test_class)]\n",
    "    score = clf.score(test_vecs, test_class)\n",
    "    \n",
    "    scores = class_data.get(gen, [])\n",
    "    scores.append(score)\n",
    "    class_data[gen] = scores\n",
    "\n",
    "    accuracies[gen] = accuracy(train_accums, v)\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "for k in class_data:\n",
    "    plt.plot(k, np.mean(class_data[k]), '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_accums)\n",
    "print(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result_files = sorted(glob('./L2L-OMNIGLOT/run_results/*.npz'))\n",
    "# result_files = sorted(glob('/home/gp283/titan/run_results/*.npz'))\n",
    "tmp = np.load(result_files[0], allow_pickle=True)\n",
    "data = {}\n",
    "class_data = {}\n",
    "for rf in result_files[-1:]:\n",
    "    \n",
    "    try:\n",
    "        tmp = np.load(rf, allow_pickle=True)\n",
    "        data.clear()\n",
    "        for k in tmp:\n",
    "            try:\n",
    "                data[k] = tmp[k].item()\n",
    "            except:\n",
    "                data[k] = tmp[k]\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    gen, ind = get_gen_ind(rf)\n",
    "    \n",
    "    n_class = data['params']['sim']['num_classes']\n",
    "    n_test = data['params']['sim']['test_per_class']\n",
    "    \n",
    "    n_train = n_test - 1\n",
    "    \n",
    "    train_vecs = data['analysis']['aggregate_per_class']['vectors'].copy()\n",
    "    train_class = np.arange(n_class, dtype='int')\n",
    "    v = data['analysis']['individual_per_class']['vectors']\n",
    "    \n",
    "    if len(v) == 0:\n",
    "        continue\n",
    "    \n",
    "#     rand_class = np.tile(np.arange(n_class), n_train)\n",
    "#     train_id = np.random.choice(len(rand_class), size=len(rand_class), replace=False)\n",
    "#     samp = train_id // n_class\n",
    "#     train_class = train_id % n_class\n",
    "#     train_vecs = [v[c][samp[i]].copy() for i, c in enumerate(train_class)]\n",
    "#     assert (n_class * n_train) == len(train_vecs), 'Not the same number of vectors'\n",
    "\n",
    "#     test_class = np.arange(n_class, dtype='int')\n",
    "#     test_vecs = [v[c][-1].copy() for c in test_class]\n",
    "    \n",
    "    rand_class = np.tile(np.arange(n_class), n_train)\n",
    "    test_id = np.random.choice(len(rand_class), size=len(rand_class), replace=False)\n",
    "    samp = test_id // n_class\n",
    "    test_class = test_id % n_class\n",
    "    test_vecs = [v[c][samp[i]].copy() for i, c in enumerate(test_class)]\n",
    "    \n",
    "    \n",
    "    scores = class_data.get(gen, [])\n",
    "    \n",
    "#     clf = SVC(kernel=\"linear\")#, C=0.025)\n",
    "#     clf = SVC(gamma=2, C=1)\n",
    "#     clf = KNeighborsClassifier()\n",
    "#     clf = linear_model.LogisticRegression()\n",
    "#     clf.fit(train_vecs, train_class)\n",
    "    score = clf.score(test_vecs, test_class)\n",
    "    \n",
    "    scores.append(score)\n",
    "    class_data[gen] = scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for k in class_data:\n",
    "    plt.plot(k, np.mean(class_data[k]), '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
